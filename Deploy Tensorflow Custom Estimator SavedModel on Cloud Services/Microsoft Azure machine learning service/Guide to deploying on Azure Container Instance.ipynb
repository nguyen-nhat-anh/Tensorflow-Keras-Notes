{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After training a custom estimator, the model is saved in an exportable format using `tf.estimator.Estimator.export_saved_model()` method (see \"Tensorflow Estimator/Custom Estimator.ipynb\" for more details).\n",
    "* SavedModel directory structure:\n",
    "![SegmentLocal](resources/1.png)\n",
    "* Model inputs and outputs:\n",
    "\n",
    "`The given SavedModel SignatureDef contains the following input(s):\n",
    "  inputs['PetalLength'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1)\n",
    "      name: Placeholder_2:0\n",
    "  inputs['PetalWidth'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1)\n",
    "      name: Placeholder_3:0\n",
    "  inputs['SepalLength'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1)\n",
    "      name: Placeholder:0\n",
    "  inputs['SepalWidth'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1)\n",
    "      name: Placeholder_1:0\n",
    "The given SavedModel SignatureDef contains the following output(s):\n",
    "  outputs['classes'] tensor_info:\n",
    "      dtype: DT_INT64\n",
    "      shape: (-1)\n",
    "      name: ArgMax:0\n",
    "  outputs['probabilities'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 3)\n",
    "      name: Softmax:0\n",
    "Method name is: tensorflow/serving/predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an overview of the deployment process:\n",
    "* Step 1: Create a workspace\n",
    "* Step 2: Register the model\n",
    "* Step 3: Define image/inference configuration\n",
    "* Step 4: Define deploy configuration\n",
    "* Step 5: Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a machine learning service workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sign in to Azure Portal https://portal.azure.com\n",
    "2. Create and configure your new workspace by providing the workspace name, subscription, resource group, and location.\n",
    " * workspace name: deploy-tensorflow-iris\n",
    " * resource group: deploy-tensorflow-iris\n",
    "3. In the newly created workspace, select <b>Overview</b> and download `config.json` file. Use this file to load the workspace configuration in your Azure ML SDK Notebook or Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering a model allows you to store, version, and track metadata about models in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to Azure Notebook server https://notebooks.azure.com and create a new project\n",
    " * project name: deploy-tensorflow-iris\n",
    "2. Upload `config.json` file and your model (the entire `export` directory)\n",
    "3. Use the following script to register your model\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config() # load workspace from config.json file\n",
    "model = Model.register(model_path = \"export\", # the path on the local file system where the model assets are located.\n",
    "                       model_name = \"tensorflow-iris\", # the name to register the model with\n",
    "                       description = \"Iris tensorflow custom estimator trained outside Azure Machine Learning service\",\n",
    "                       workspace = ws) # the workspace to register the model under\n",
    "```\n",
    "4. After registering the model, go back to the portal. Select your workspace and then select `Assets/Models` to check whether the model is registered successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define image/inference configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image/inference configuration describes how to configure the model to make predictions. This configuration specifies the runtime, the entry script, and (optionally) the conda environment file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration references the following files that are used to run the model when it's deployed:\n",
    "* The runtime. The only valid value for runtime currently is Python.\n",
    "* An entry script. This file (named `score.py`) loads the model when the deployed service starts. It is also responsible for receiving data, passing it to the model, and then returning a response.\n",
    "\n",
    "  The script contains two functions that load and run the model:\n",
    "\n",
    "  * init(): Typically this function loads the model into a global object. This function is run only once when the Docker container for your web service is started.\n",
    "\n",
    "  * run(input_data): This function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization. You can also work with raw binary data. You can transform the data before sending to the model, or before returning to the client.\n",
    "\n",
    "  The following is the Python code for the entry script `score.py`. \n",
    "\n",
    "```python\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the deployed service starts\n",
    "def init():\n",
    "    \"\"\"\n",
    "    `inputs` and `outputs` are dictionaries of tensors.\n",
    "    `inputs` has the following format:\n",
    "    {\n",
    "        \"SepalWidth\": corresponding placeholder,\n",
    "        \"SepalLength\": corresponding placeholder,\n",
    "        \"PetalLength\": corresponding placeholder,\n",
    "        \"PetalWidth\": corresponding placeholder\n",
    "    }\n",
    "    `outputs` has the following format:\n",
    "    {\n",
    "        \"probabilities\": corresponding tensor,\n",
    "        \"classes\": corresponding tensor\n",
    "    }\n",
    "    \"\"\"\n",
    "    global inputs, outputs, sess # These variables are global because the `run()` function uses them \n",
    "    model_dir = Model.get_model_path('tensorflow-iris') # return the path to the model \n",
    "                                                        # registered with the name `tensorflow-iris`\n",
    "    # load the model, and then load the input and output tensors into `inputs` and `outputs` variables\n",
    "    sess = tf.Session()\n",
    "    meta_graph_def = tf.saved_model.load(sess, [tf.saved_model.tag_constants.SERVING], model_dir)\n",
    "    signature_def = meta_graph_def.signature_def['serving_default']\n",
    "    inputs = {}\n",
    "    for k in signature_def.inputs.keys():\n",
    "        inputs[k] = tf.get_default_graph().get_tensor_by_name(signature_def.inputs[k].name)\n",
    "    outputs = {}\n",
    "    for k in signature_def.outputs.keys():\n",
    "        outputs[k] = tf.get_default_graph().get_tensor_by_name(signature_def.outputs[k].name)\n",
    "\n",
    "# Handle requests to the service\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    `raw_data` has the following format:\n",
    "    {\n",
    "        \"SepalWidth\": [1d-array],\n",
    "        \"SepalLength\": [1d-array],\n",
    "        \"PetalLength\": [1d-array],\n",
    "        \"PetalWidth\": [1d-array]\n",
    "    }\n",
    "    \"\"\"\n",
    "    data_dict = json.loads(raw_data)\n",
    "    feed_dict = {}\n",
    "    for k in data_dict.keys():\n",
    "        feed_dict[inputs[k]] = np.array(data_dict[k])\n",
    "    # make prediction\n",
    "    probs, classes = sess.run([outputs['probabilities'], outputs['classes']], feed_dict=feed_dict)\n",
    "    # you can return any datatype as long as it is JSON-serializable\n",
    "    return {'probs': probs.tolist(), 'classes': classes.tolist()} # numpy array are not JSON-serializable\n",
    "                                                                  # so have to convert to list first\n",
    "```\n",
    "* A conda environment file. This file defines the Python packages needed to run the model and entry script.\n",
    "\n",
    "  The following YAML describes the conda environment needed to run the model and entry script:\n",
    "  \n",
    "  `myenv.yml`\n",
    "  \n",
    "  ```yaml\n",
    "  name: project_environment\n",
    "  dependencies:\n",
    "  - python=3.6.2\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "  - tensorflow=1.13\n",
    "  - numpy\n",
    "  channels:\n",
    "  - conda-forge\n",
    "  ```\n",
    "\n",
    "  Python script to generate `myenv.yml`:\n",
    "  \n",
    "```python\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_tensorflow_conda_package(core_type='cpu', version='1.13')\n",
    "myenv.add_conda_package('numpy')\n",
    "\n",
    "with open('myenv.yml', 'w') as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the image configuration.\n",
    "\n",
    "```python\n",
    "from azureml.core.image import ContainerImage\n",
    "image_config = ContainerImage.image_configuration(execution_script='score.py',\n",
    "                                                 runtime='python',\n",
    "                                                 conda_file='myenv.yml')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define deploy configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deploying, you must define the deployment configuration. The deployment configuration is specific to the compute target that will host the web service.\n",
    "\n",
    "The following table provides an example of creating a deployment configuration for each compute target:\n",
    "\n",
    "| Compute target | Deployment configuration example |\n",
    "| --- | --- |\n",
    "| Local | `deployment_config = LocalWebservice.deploy_configuration(port=8890)` |\n",
    "| Azure Container Instance | `deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)` |\n",
    "| Azure Kubernetes Service | `deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)` |\n",
    "\n",
    "In this example, we will deploy the model on Azure Container Instance:\n",
    "\n",
    "```python\n",
    "from azureml.core.webservice import AciWebservice\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During deployment, the image/inference configuration and deployment configuration are used to create and configure the service environment.\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(workspace=ws, name='tensorflow-iris')\n",
    "service = Webservice.deploy_from_model(workspace=ws,\n",
    "                                      name='tensorflow-iris',\n",
    "                                      deployment_config=deployment_config,\n",
    "                                      models=[model],\n",
    "                                      image_config=image_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request-response consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After deployment, the scoring URI is displayed. This URI can be used by clients to submit requests to the service.\n",
    "\n",
    "* Note: How to get the scoring URI\n",
    " * Go to the portal\n",
    " * Select your machine learning service workspace\n",
    " * Select `Assets/Deployments`\n",
    " * Choose your model and get the scoring URI\n",
    "\n",
    "* Create a POST request with the following body\n",
    "```json\n",
    "{\n",
    "    \"SepalWidth\": [\n",
    "        2.8\n",
    "    ],\n",
    "    \"SepalLength\": [\n",
    "        6.4\n",
    "    ],\n",
    "    \"PetalLength\": [\n",
    "        5.6\n",
    "    ],\n",
    "    \"PetalWidth\": [\n",
    "        2.2\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "* Use Postman to test the api\n",
    "![SegmentLocal](resources/2.png)\n",
    "\n",
    "* Result\n",
    "![SegmentLocal](resources/3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
