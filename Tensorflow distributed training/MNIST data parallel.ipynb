{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of physical devices visible to the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "cpus = tf.config.experimental.list_physical_devices('CPU') # get a list of cpus\n",
    "print(cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the CPU into 2 virtual devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:1', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(cpus[0],\n",
    "                                                            [tf.config.experimental.VirtualDeviceConfiguration(),\n",
    "                                                             tf.config.experimental.VirtualDeviceConfiguration()])\n",
    "    virtual_cpus = tf.config.experimental.list_logical_devices('CPU')\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "print(virtual_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1219 14:51:47.002589 19160 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[cpu.name for cpu in virtual_cpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 128\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync # 128 * 2 = 256\n",
    "N_CLASSES = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(x, y):\n",
    "    '''\n",
    "    Normalize input images and one-hot encode the labels \n",
    "    '''\n",
    "    return (tf.cast(x, tf.float32) / 255., tf.one_hot(y, N_CLASSES))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(GLOBAL_BATCH_SIZE).map(preprocess_fn)\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    iterator = train_dist_dataset.make_initializable_iterator()\n",
    "    iterator_init = iterator.initialize()\n",
    "    next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple neural network for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_model():\n",
    "    input_layer = tf.keras.Input(shape=(28, 28)) # (None, 28, 28)\n",
    "    x = tf.keras.layers.Flatten()(input_layer) # (None, 28*28)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x) # (None, 1024)\n",
    "    output_layer = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(x) # (None, N_CLASSES)\n",
    "    \n",
    "    return tf.keras.Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and optimizer inside the strategy's scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1219 14:51:48.413801 19160 deprecation.py:506] From f:\\anaconda3\\envs\\tensorflow1.14\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = mnist_model()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def train_step(inputs):\n",
    "        '''\n",
    "        inputs: \"per-replica\" values, such as those produced by a \"distributed Dataset\"\n",
    "        '''\n",
    "        images, labels = inputs\n",
    "        preds = model(images)\n",
    "        per_sample_loss = tf.keras.losses.CategoricalCrossentropy(reduction='none')(labels, preds) # (batch_size,)\n",
    "        loss = tf.reduce_sum(per_sample_loss) / tf.cast(GLOBAL_BATCH_SIZE, per_sample_loss.dtype) # scalar\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # make sure `loss` will only be returned after `train_op` have executed\n",
    "        with tf.control_dependencies([train_op]): \n",
    "            return tf.identity(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: `tf.control_dependencies` documentation explicitly stated that \"The control dependencies context applies only to ops that are constructed within the context. Merely using an op or tensor in the context does not add a control dependency\". So we have to use `tf.identity` as a workaround (to make an extra op within the control dependency context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def distributed_train_step(dataset_inputs):\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step, args=(dataset_inputs,))\n",
    "        mean_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses)\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished! - Loss: 2.07 - Time elapsed: 3.65\n",
      "Epoch 1 finished! - Loss: 1.76 - Time elapsed: 3.26\n",
      "Epoch 2 finished! - Loss: 1.60 - Time elapsed: 3.27\n",
      "Epoch 3 finished! - Loss: 1.40 - Time elapsed: 3.22\n",
      "Epoch 4 finished! - Loss: 1.26 - Time elapsed: 3.37\n",
      "Epoch 5 finished! - Loss: 1.16 - Time elapsed: 3.39\n",
      "Epoch 6 finished! - Loss: 1.07 - Time elapsed: 3.29\n",
      "Epoch 7 finished! - Loss: 1.01 - Time elapsed: 3.46\n",
      "Epoch 8 finished! - Loss: 0.98 - Time elapsed: 3.37\n",
      "Epoch 9 finished! - Loss: 0.88 - Time elapsed: 3.35\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    global_init = tf.global_variables_initializer()\n",
    "    \n",
    "    loss_op = distributed_train_step(next_element)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(global_init)\n",
    "        for epoch in range(EPOCHS):\n",
    "            sess.run(iterator_init)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    loss_result = sess.run(loss_op)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print('Epoch {} finished! - Loss: {:.2f} - Time elapsed: {:.2f}'.format(epoch, loss_result, time.time()-start))\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
